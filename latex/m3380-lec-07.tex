\documentclass[m3380-lec-main.tex]{subfiles}
\setcounter{chapter}{6}

%\DeclareMathOperator{\R}{\mathbb{R}}

\begin{document}


\chapter{Matrix decomposition: $LU$ decomposition}

\section*{Goals}
\begin{enumerate}[1.~]\setlength{\itemsep}{0pt}
\item Discuss elementary matrices and their relationship to Gaussian elimination
\item Discuss matrix inverses
\item Discuss the $LU$ decomposition of an invertible matrix $A$
\item Discuss the $PA=LU$ factorization of an invertible matrix $A$
\end{enumerate}

\begin{rem}We often talk about square matrices in the context of systems of linear equations because a system of fewer than $n$ equations in $n$ unknowns is \emph{underqualified}; that is, there are too few equations to give a unique solution in all variables. On the other hand, systems of more than $n$ equations in $n$ variables either have redundant equations (at least one equations is a linear combination of the others) or the system is \emph{overqualified} and an inconsistent system. Thus $n$ equations in $n$ unknowns is the sort of ``happy medium" of having enough equations for unique solutions without having too many for a solution to exist.\end{rem}

\section{Elementary matrices}

Suppose there is a matrix $A$, and some sequence of row operations transforms it into $A'$, so that $A\to A'$. What happens if we assume that there is some matrix $E$ such that $EA=A'$? Assuming that such an $E$ exists, what can we say about it? 

It makes sense to think that nothing is special about the matrix $A$ when performing a row operation; for instance, $-2R_1+R_3$ always produces elements $a_{3,j}-2a_{1,j}$ in the third row, no matter what the values of $a_{i,j}$ are. So we should think that if $E$ corresponds to a particular elementary row operation, then left-multiplying \emph{any} matrix of the correct dimensions by $E$ will have the proper result. Further, $EI = E$, so we know that whatever effect a row operation has on the identity matrix results in the elementary matrix itself!

\subsection{Row swaps} Consider then the result on the $4\times 4$ identity matrix of having rows $1$ and $3$ swapped.
\[ \begin{bmr}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmr} \xto{R_1:R_3} 
	\begin{bmr}0 & 0 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1\end{bmr}.\]
Hence the matrix corresponding to interchanging rows $k$ and $\ell$ must be the matrix with 0s in all entries except for 1s when $i=j$ and $i\neq k$, $i\neq \ell$, and $a_{k,\ell}=a_{\ell,k}=1$. That is, $E=[e_{i,j}]$ with
\[ e_{i,j} = \begin{cases} 
	1, & i=j\text{ and }i,j\notin\set{k,\ell}\text{, or }i\neq j\text{ and }i,j\in\set{k,\ell} \\
	0, & \text{otherwise}.
\end{cases}\]
%Then if $A=[a_{i,j}]$ is an $n\times n$ matrix and $E$ is the matrix which interchanges rows $k$ and $\ell$ of the identity matrix, 
%\begin{align*}
%EA &= 
		
\subsection{Scalar multiplication of a row} Again, operating on the $4\times 4$ identity matrix gives insight to the structure of this elementary matrix:
\[  \begin{bmr}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmr} \xto{2R_4} 
	\begin{bmr}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 2\end{bmr}. \]
Hence the $n\times n$ matrix which corresponds to multiplying row $k$ by a scalar $\lambda$ is the matrix $E=[e_{i,j}]$ where
\[ e_{i,j} = \begin{cases}
	1, & i=j\text{ and } i\neq k\\
	\lambda, & i=j=k \\
	0, &\text{otherwise.}
\end{cases}\]

\subsection{Adding a scalar multiple of one row to another row} Once again, consider the $4\times 4$ identity matrix:
\[  \begin{bmr}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmr} \xto{9R_4+R_2} 
	\begin{bmr}1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 9 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1\end{bmr}. \]
Hence the $n\times n$ matrix which corresponds to adding to each column of row $\ell$ the scalar multiple $\lambda$ times the corresponding column of row $k$ ($\lambda R_k+R_{\ell}$) is the matrix $E=[e_{i,j}]$ where
\[ e_{i,j} = \begin{cases}
	1, & i=j \\
	\lambda, & i=\ell\text{ and }j=k\\
	0, &\text{otherwise}.
\end{cases}\]
Note there that the value $\lambda$ appears in the row {being added to} and in the column corresponding to the row which is {being multiplied}.

\begin{thm} Suppose that $A$ is a $n\times n$ matrix and that there is some sequence of elementary row operations by which $A$ can be transformed into $I$, the $n\times n$ identity matrix. Then there is a sequence $(E_1,E_2,\ldots,E_k)$ of elementary matrices such that \[E_kE_{k-1}\cdots E_2E_1A=I.\]
\hfill$\qedsymbol$
\end{thm}

\section{Matrix inverses}
\begin{defn} Let $A$ be an $n\times n$ matrix. Suppose there is a matrix $B$ such that $AB=I$. Then $B$ is the \emph{inverse} of $A$, and we write $A^{-1}$ instead of $B$. Moreover, $A^{-1}$ is the unique such matrix and $A^{-1}A = I = AA^{-1}$.
\end{defn}

\begin{thm} Suppose $A$ is a $n\times n$ matrix and there is a sequence $(E_1,E_2,\ldots,E_k)$ of elementary matrices such that \[E_kE_{k-1}\cdots E_2E_1A=I.\] Then $A^{-1}=E_kE_{k-1}\cdots E_2E_1$.
\end{thm}
\begin{proof} From the definition and Theorem 7.1.\end{proof}

\subsection{Application of Gauss-Jordan elimination}
We find the inverse of a matrix by the same process of Gauss-Jordan elimination as used when solving systems of equations, but now we augment our square matrix $A$ by an appropriately sized identity matrix rather than by a single vector.

\begin{exmp} Suppose we begin with a $3\times 3$ matrix augmented by $I$
\[A = \begin{augbmr}{3}{3}
	1 & 3 & 5 & 1 & 0 & 0 \\
	7 & 9 & 11 & 0 & 1 & 0 \\
	2 & -4 & -6 & 0 & 0 & 1
	\end{augbmr}.\]
Just to demonstrate that the choice of elementary row operations does not matter, we'll use Gauss-Jordan elimination with partial pivoting. Row operations marked on arrows are performed in order from the top to the bottom.

\begin{align*}
\begin{augbmr}{3}{3}
	1 & 3 & 5 & 1 & 0 & 0 \\
	7 & 9 & 11 & 0 & 1 & 0 \\
	2 & -4 & -6 & 0 & 0 & 1
	\end{augbmr} \xrightarrow[\begin{smallmatrix}-R_1+R_2\\-2R_1+R_3\end{smallmatrix}]{\begin{smallmatrix}R_1:R_2\\\frac17 R_1\end{smallmatrix}}
	&\begin{augbmr}{3}{3}
	1 & \frac97 & \frac{11}7 & 0 & \frac17 & 0 \\
	0 & \frac{12}7 & \frac{24}7 & 1 & -\frac17 & 0 \\
	0 & -\frac{46}7 & -\frac{64}7 & 0 & -\frac27 & 1
	\end{augbmr} \\
	\xrightarrow[\begin{smallmatrix}-\frac97R_2+R_1 \\ -\frac{12}7R_2+R_3\end{smallmatrix}]{\begin{smallmatrix}R_2:R_3\\-\frac{7}{46}R_2\end{smallmatrix}}
	&\begin{augbmr}{3}{3}
	1 & 0 & -\frac5{23} & 0 & \frac2{23} & \frac9{46} \\
	0 & 1 & \frac{32}{23} & 0 &  \frac1{23} &-\frac7{46} \\
	0 & 0 & \frac{24}{23} & 1 & -\frac5{23} & \frac6{23}
	\end{augbmr} \\
	\xrightarrow[\begin{smallmatrix}\frac5{23}R_3+R_1\\-\frac{32}{23}R_3+R_2\end{smallmatrix}]{\frac{23}{24}R_3}
	&\begin{augbmr}{3}{3}
	1 & 0 & 0 & \frac5{24} & \frac1{24} & \frac14 \\
	0 & 1 & 0 & -\frac43 & \frac13 & -\frac12 \\
	0 & 0 & 1 & \frac{23}{24} & -\frac5{24} & \frac14
	\end{augbmr}
\end{align*}
Now, observe:
\begin{align*}
%\begingroup \renewcommand*{\arraystretch}{1.2}
	\left[\begin{smallmatrix} 
		1 & 3 & 5 \\
		7 & 9 & 11 \\
		2 & -4 & 6
	\end{smallmatrix}\right]
	\left[\begin{smallmatrix} 
		\frac5{24} & \frac1{24} & \frac14 \\
		-\frac43 & \frac13 & -\frac12 \\
		\frac{23}{24} & -\frac5{24} & \frac14
	\end{smallmatrix}\right] &=
	\left[\begin{smallmatrix}
	1\left(\frac5{24}\right)+3\left(-\frac43\right)+5\left(\frac{23}{24}\right) & 
	1\left(\frac1{24}\right)+3\left(\frac13\right)+5\left(-\frac5{24}\right) &
	1\left(\frac14\right)+3\left(-\frac12\right)+5\left(\frac14\right) \\
%
	7\left(\frac5{24}\right)+9\left(-\frac43\right)+11\left(\frac{23}{24}\right) & 
	7\left(\frac1{24}\right)+9\left(\frac13\right)+11\left(-\frac5{24}\right) &
	7\left(\frac14\right)+9\left(-\frac12\right)+11\left(\frac14\right) \\
%
	2\left(\frac5{24}\right)-4\left(-\frac43\right)-6\left(\frac{23}{24}\right) & 
	2\left(\frac1{24}\right)-4\left(\frac13\right)-6\left(-\frac5{24}\right) &
	2\left(\frac14\right)-4\left(-\frac12\right)-6\left(\frac14\right) \\
%
	\end{smallmatrix}\right] \\
	&=
	\begin{bmr}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmr}
%	\endgroup
\end{align*}
\end{exmp}
\section{$LU$ decomposition} 
\begin{defn} A $m\times n$ matrix $L$ is \emph{lower triangular} if its entries satisfy $\ell_{i,j}=0$ for $j>i$ (entries to the right of the ``main diagonal" are zero). A $m\times n$ matrix $U$ is \emph{upper triangular} if its entries satisfy $u_{i,j} = 0$ for $j<i$ (entries to the left of the ``main diagonal" are zero). An $LU$ decomposition of a matrix $A$ is any factorization $A=LU$ where $L$ is lower triangular and $U$ is an upper triangular row echelon form of $A$.
\end{defn}
We can apply our theory of elementary matrices to this problem, immediately!

\begin{exmp} Recall from Lesson 6 our example system of equations and its corresponding augmented matrix form:
\begin{align*}
&\systeme{
x_1 + 3x_2 - 13x_3 = -18, 
x_1 + 2x_2 - 10x_3 = -15, 
-6x_1 - 7x_2 + 46x_3 = \phantom{-}77
}  &
A\vert\vec{b} &= \begin{abmr}{3} 1 & 3 & -13 & -18 \\ 1 & 2 & -10 & -15\\ -6 & -7 & 46 & 77 \end{abmr}.
\end{align*}
The three required steps were $\xto{-R_1+R_2}$, $\xto{6R_1+R_3}$, and $\xto{11R_2+R_3}$, resulting in 
\[U = \begin{bmr} 1 & 3 & -13\\ 0 & -1 & 3\\ 0 & 0 & 1\end{bmr}\]
when applied to just $A$. So if we let those correspond to elementary matrices $E_1$, $E_2$, and $E_3$ respectively, we get
\[E_3E_2E_1A = \begin{bmr}1&0&0\\-1&1&0\\6&11&1\end{bmr}\begin{bmr}1&3&-13\\1&2&-10\\-6&-7&46\end{bmr}
	= \begin{bmr} 1 & 3 & -13\\ 0 & -1 & 3\\ 0 & 0 & 1\end{bmr} = U.\]
If we left-multiply both sides of this equation with the correct inverse matrices and obtain a lower triangular matrix as their product, we'll be done!
\end{exmp}

At this point it becomes helpful to assign special letters to the elementary matrices; these are by no means standard throughout the linear algebra literature and serve mostly to make clear the following theorem. Generally, when you discuss elementary matrices, they will simply be $E_i$s.

\begin{thm} All elementary matrices are invertible. 
\begin{enum}
\item If $S_{k,\ell}$ is an elementary matrix which swaps rows $k$ and $\ell$ ($R_k:R_\ell$), then $S_{k,\ell}^2=I$, and $S_{k,\ell}=S_{k,\ell}^{-1}$.
\item If $M_{\lambda, k}$ is an elementary matrix which multiplies row $k$ by a scalar $\lambda$ ($\lambda R_k$), then $M_{\lambda,k}^{-1} = M_{1/\lambda, k}$.
\item If $E_{\lambda, k, j}$ is an elementary matrix which multiplies row $k$ by a scalar $\lambda$ and then adds the result onto row $\ell$ ($\lambda R_k+R_\ell$), then $E_{\lambda, k, j}^{-1} = E_{-\lambda, k, j}$.
\end{enum}
\end{thm}

\begin{exmp}
Continuing from above, we have 
\[E_3E_2E_1 = \begin{bmr}1&0&0\\-1&1&0\\6&11&1\end{bmr},\]
and all of $E_1, E_2, and E_3$ are matrices as in Theorem 7.7 part 3. Hence 
\[(E_3E_2E_1)^{-1}=E_1^{-1}E_2^{-1}E_3^{-1} = \begin{bmr}1&0&0\\1&1&0\\-6&-11&1\end{bmr}.\]
Since this is lower triangular, we say $L=E_1^{-1}E_2^{-1}E_3^{-1}$ and have obtained $A=LU$, the $LU$ factorization of our matrix $A$. Now we need to use the $LU$ decomposition to solve the system of equations!
\end{exmp}

Matrix algebra very quickly gives us a method of solving the matrix equation $A\vec{x}=\vec{b}$ using the $LU$ decomposition: 
\[\vec{b} = A\vec{x} = (LU)\vec{x} = L(U\vec{x}).\]
So we now can separate this into two problems, namely $L\vec{y} = \vec{b}$ and $U\vec{x} = \vec{y}$, but these are both triangular matrices. The solution to a this type of problem with triangular matrices involves only substitutions (either forward or backward)!

\begin{exmp} Let's take our example and finish it out with substitution. 
\begin{align*}
A &= \begin{bmr} 1 & 3 & -13 \\ 1 & 2 & -10\\ -6 & -7 & 46 \end{bmr} &
L &= \begin{bmr}1 & 0 & 0 \\ 1 & 1 & 0 \\ -6 & -11 & 1\end{bmr} &
U &= \begin{bmr} 1 & 3 & -13\\ 0 & -1 & 3\\ 0 & 0 & 1\end{bmr}
\end{align*}
Recall that we must solve $L\vec{y}=\vec{b}$ before we can solve $U\vec{x}=\vec{y}$, and so doing we obtain
\begin{align*}
y_1 &= b_1 				& x_3 &= y_3 \\
y_2 &= b_2-y_1 			& x_2 &= -(y_2-3x_3) \\
y_3 &= b_3+6y_1+11y_2	& x_1 &= y_1-3x_2+13x_3
\end{align*}
As you can see, we can now solve the matrix equation $A\vec{x}=\vec{b}$ very quickly for any ${\vec{b}=\vc{b_1,b_2,b_3}}$. In our example, we had $\vec{b}=\vc{-18,-15,77}$, so we get
\begin{align*}
y_1 = b_1 &= -18				& x_3 = y_3 &= 2 \\
y_2 = b_2-y_1 &= 3				& x_2 = -(y_2-3x_3) &= 3\\
y_3 = b_3+6y_1+11y_2 &= 2		& x_1 = y_1-3x_2+13x_3 &= -1
\end{align*}
and our solution vector is $\vec{x} = \vc{2,3,-1}$.
\end{exmp}


\subsection{Motivation for using $LU$ decomposition: operational complexity}

We have so far avoided discussing algorithmic complexity, but it is no longer avoidable.

\begin{defn} Suppose that an algorithm requires $f(n)$ operational steps to complete when the input has size $n$\footnote{Both of these are very vague, because individual operations differ between algorithms, as does the measurement of size of input.}. Suppose also that there is some function $F(n)$ such that 
\[ \lim_{n\to\infty} \frac{F(n)}{f(n)} = c \] is constant. Then we say the algorithm has a \emph{big-O complexity} of $F(n)$, written $O(F(n))$\footnote{While any function $F$ will do, we try to find as simple a function as possible while keeping the limit positive.}.
\end{defn}

This applies to the problem of solving systems of equations like so: a very well implemented naive algorithm for producing the Gaussian elimination of a matrix requires $\frac23n^3+\frac12n^2-\frac76n$ divisions, multiplications, and addition/subtractions to perform the elimination step, and then an additional $n^2$ of these operations to perform the back substitution step. Hence the operational complexity of naive Gaussian elimination is $O(n^3)$, since
\[ \lim_{n\to\infty}\frac{\frac23n^3+\frac12n^2-\frac76n}{n^3} = \frac23. \]

What if instead of solving an equation $A\vec{x}=\vec{b}$, we actually have to solve a system
\begin{align*}
A\vec{x}&=\vec{b}_1, &
A\vec{x}&=\vec{b}_2, &
A\vec{x}&=\vec{b}_3, &
&\cdots &
A\vec{x}&=\vec{b}_n,
\end{align*}
where $A$ is a $n\times n$ matrix? In such a case, since our vectors $\vec{b}_i$ are involved in every step of each Gaussian elimination, solving all $n$ problems becomes a $O(n^4)$ algorithm, which is much worse than $O(n^3)$.

On the other hand, the operational complexity of finding the $LU$ decomposition of $A$ is still $O(n^3)$, since we do all the same steps but have some additional assignments into the matrix $L$. Using the $LU$ decomposition to solve $(LU)\vec{x}=\vec{b}_i$ requires $2n^2$ operations for the forward substitution involved in solving $L\vec{y}=\vec{b}_i$ and then the back substitution of solving $U\vec{x}=\vec{y}$. Using the $LU$ decomposition to solve the system of matrix equations above then is still $O(n^3)$, since
\[\lim_{n\to\infty}\frac{n^3+2n^2(n)}{n^3} = 3.\]

To sum up, the $LU$ decomposition provides no net gain when used to solve a single matrix equation, but when faced with a large system of matrix equations it provides an enormous benefit in terms of the complexity of the algorithm and the time it will require for a program to complete the process.

%\section{$QR$ decomposition}
%The $QR$ decomposition of a $m\times n$ matrix $A$ serves an entirely different purpose than solving systems of equations; its principal use is in finding the least squares regression approximation to a set of data points in the plane. This is a problem dating from the work of Gauss and Legendre in the early 1800s.
%
\section{$PA=LU$ factorization} All of the above work is for naught if there is a zero pivot, of course, or computationally if due to swamping we encounter insurmountable errors. The solution, of course, is to use partial pivoting -- but permuting the rows of the matrix introduces a complication into the process. The solution is to find the appropriate \emph{permutation matrix $P$} so that $PA=LU$.

\begin{defn} A matrix $P$ is a \emph{permutation matrix} if it is a product of elementary matrices of type $S_{k,\ell}$ only.
\end{defn}

In the $LU$ factorization, the matrix $L$ is used as a place to ``store" the row multipliers which are used in elimination; a more clever implementation of the algorithm allows those values to be stored in the matrix $U$ below the main diagonal! This is useful as it allows the rows to be pivoted and the scalar multipliers to stay with their correct row if rows need to be swapped for partial pivoting. The row interchanges will be tracked in a permutation matrix $P$.

\begin{exmp}
We will decompose a different example than previously to demonstrate how the multipliers stored within the matrix move with their rows. Boxed entries are ``zeroes," which for sake of convenience will have the multipliers stored in them.
\begin{align*}
A = \begin{bmr} 
		4 & 9 & -11 \\ 
		-2 & 3 & 5 \\ 
		7 & 1 & 1
	\end{bmr}
	\xrightarrow[R_1:R_3]{P=\left[\begin{smallmatrix}0&0&1\\0&1&0\\1&0&0\end{smallmatrix}\right]}
	&\begin{bmr} 
		7 & 1 & 1 \\
		-2 & 3 & 5 \\ 
		4 & 9 & -11 
	\end{bmr} 
	\xrightarrow[R_3-\frac47R_1]{R_2-(-\frac27 R_1)}
	\begin{bmr}
		7 & 1 & 1 \\
		\fbox{$-\frac27$} & \frac{23}7 & \frac{37}7 \\
		\fbox{$\frac47$} & \frac{59}7 & -\frac{81}7
	\end{bmr} \\
	\xrightarrow[R_2:R_3]{P=\left[\begin{smallmatrix}0&0&1\\1&0&0\\0&1&0\end{smallmatrix}\right]}
	&\begin{bmr}
		7 & 1 & 1 \\
		\fbox{$\frac47$} & \frac{59}7 & -\frac{81}7 \\
		\fbox{$-\frac27$} & \frac{23}7 & \frac{37}7 
	\end{bmr}
	\xto{R_3 - \frac{23}{59}R_2}
	\begin{bmr}
		7 & 1 & 1 \\
		\fbox{$\frac47$} & \frac{59}7 & -\frac{81}7 \\
		\fbox{$-\frac27$} & \fbox{$\frac{23}{59}$} & \frac{578}{59} 
	\end{bmr}
\end{align*}
Pulling apart this very compact notation, we have 
\begin{align*}
P &= \begin{bmr}0&0&1\\1&0&0\\0&1&0\end{bmr} &
A &= \begin{bmr} 4 & 9 & -11 \\ -2 & 3 & 5 \\ 7 & 1 & 1 \end{bmr} &
L &= \begin{bmr}1 & 0 & 0 \\ \frac47 & 1 & 0 \\ -\frac27 & \frac{23}{59} & 1\end{bmr} &
U &= \begin{bmr}7 & 1 & 1 \\ 0 & \frac{23}7 & -\frac{81}7 \\ 0 & 0 & \frac{578}{59}\end{bmr}
\end{align*}
and can very easily check
\[PA = \begin{bmr}7 & 1 & 1 \\ 4 & 9 & -11 \\ -2 & 3 & 5\end{bmr} = LU.\]
\end{exmp}

Here's  code that can be embedded into \verb|Matrix.py| to implement the $PA=LU$ factorization of a matrix $A$. This does not use any of the elementary row operation methods previously developed

\smallskip{\fn\begin{verbatim}
    def palu(self):
        m,n = self.dims()
        if m!=n:
            raise ValueError('Nonsquare matrix.')
        elif self.det() == 0:
            raise ValueError('Singular matrix.')
        else:
            P = [[1 if i==j else 0 for j in range(n)] for i in range(m)]
            mat = [[self[i,j] for j in range(n)] for i in range(m)]
            for j in range(m):
                maxv = abs(mat[j][j])
                piv = j
                for i in range(j+1,m):
                    if abs(mat[i][j]) > maxv:
                        maxv = mat[i][j]
                        piv = i
                if piv != j:
                    mat = mat[:j] + [mat[piv]] + mat[j+1:piv] + [mat[j]] \
                          + mat[piv+1:]
                    P = P[:j] + [P[piv]] + P[j+1:piv] + [P[j]] + P[piv+1:]
                for i in range(j+1,m):
                    mult = Fraction(mat[i][j],mat[j][j])
                    mat[i][j] = mult
                    for c in range(j+1,n):
                        mat[i][c] += -mult*mat[j][c]
            L = Matrix([[1 if i==j else (0 if j>i else mat[i][j])
                         for j in range(n)] for i in range(m)])
            U = Matrix([[mat[i][j] if j>=i else 0 for j in range(n)]
                        for i in range(m)])
            return (Matrix(P),L,U)
\end{verbatim}
}\smallskip\noindent




\end{document}